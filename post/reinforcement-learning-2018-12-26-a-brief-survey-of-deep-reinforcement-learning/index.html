<!DOCTYPE html>
<html lang="en" itemscope itemtype="http://schema.org/WebPage">
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>[深度强化学习概述]A Brief Survey of Deep Reinforcement Learning - Pan Feng&#39;s Blog</title>
  

<meta name="renderer" content="webkit" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>

<meta name="MobileOptimized" content="width"/>
<meta name="HandheldFriendly" content="true"/>


<meta name="applicable-device" content="pc,mobile">

<meta name="theme-color" content="#f8f5ec" />
<meta name="msapplication-navbutton-color" content="#f8f5ec">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="#f8f5ec">

<meta name="mobile-web-app-capable" content="yes">

<meta name="author" content="Pan Feng" />
  <meta name="description" content="[深度强化学习概述] A Brief Survey of Deep Reinforcement Learning 原文参考: arXiv:1708.05866v2 [cs.LG] 28 Sep 2017 摘要 本文主要内容有： 研究深度强化学习的核心算法 deep Q-network (深度Q网络) trust region policy optimisation (置信区间策" />







<meta name="generator" content="Hugo 0.52" />


<link rel="canonical" href="http://panfeng0119.github.io/post/reinforcement-learning-2018-12-26-a-brief-survey-of-deep-reinforcement-learning/" />



<link rel="icon" href="/favicon.ico" />











<link rel="stylesheet" href="/sass/jane.min.c8c1ff75dee09b44060a6c38f41b9036bac2512ccdb22d7f296cec12dc786375.css" integrity="sha256-yMH/dd7gm0QGCmw49BuQNrrCUSzNsi1/KWzsEtx4Y3U=" media="screen" crossorigin="anonymous">





<meta property="og:title" content="[深度强化学习概述]A Brief Survey of Deep Reinforcement Learning" />
<meta property="og:description" content="[深度强化学习概述] A Brief Survey of Deep Reinforcement Learning 原文参考: arXiv:1708.05866v2 [cs.LG] 28 Sep 2017 摘要 本文主要内容有： 研究深度强化学习的核心算法 deep Q-network (深度Q网络) trust region policy optimisation (置信区间策" />
<meta property="og:type" content="article" />
<meta property="og:url" content="http://panfeng0119.github.io/post/reinforcement-learning-2018-12-26-a-brief-survey-of-deep-reinforcement-learning/" /><meta property="article:published_time" content="2018-12-26T15:35:02&#43;08:00"/>
<meta property="article:modified_time" content="2018-12-26T15:35:02&#43;08:00"/>

<meta itemprop="name" content="[深度强化学习概述]A Brief Survey of Deep Reinforcement Learning">
<meta itemprop="description" content="[深度强化学习概述] A Brief Survey of Deep Reinforcement Learning 原文参考: arXiv:1708.05866v2 [cs.LG] 28 Sep 2017 摘要 本文主要内容有： 研究深度强化学习的核心算法 deep Q-network (深度Q网络) trust region policy optimisation (置信区间策">


<meta itemprop="datePublished" content="2018-12-26T15:35:02&#43;08:00" />
<meta itemprop="dateModified" content="2018-12-26T15:35:02&#43;08:00" />
<meta itemprop="wordCount" content="2023">



<meta itemprop="keywords" content="reinforcement learning," />
<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="[深度强化学习概述]A Brief Survey of Deep Reinforcement Learning"/>
<meta name="twitter:description" content="[深度强化学习概述] A Brief Survey of Deep Reinforcement Learning 原文参考: arXiv:1708.05866v2 [cs.LG] 28 Sep 2017 摘要 本文主要内容有： 研究深度强化学习的核心算法 deep Q-network (深度Q网络) trust region policy optimisation (置信区间策"/>

<!--[if lte IE 9]>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js"></script>
<![endif]-->

<!--[if lt IE 9]>
  <script src="https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js"></script>
<![endif]-->



  <link href='http://panfeng0119.github.io/avatar/favicon.ico' rel='icon' type='image/x-icon'/>


</head>
<body>
  <div id="mobile-navbar" class="mobile-navbar">
  <div class="mobile-header-logo">
    <a href="/" class="logo">Pan Feng</a>
  </div>
  <div class="mobile-navbar-icon">
    <span></span>
    <span></span>
    <span></span>
  </div>
</div>
<nav id="mobile-menu" class="mobile-menu slideout-menu">
  <ul class="mobile-menu-list">
    <li class="mobile-menu-item">
        
          
          
            <a class="menu-item-link" href="http://panfeng0119.github.io/">Home</a>
          
        
      </li><li class="mobile-menu-item">
        
          
          
            <a class="menu-item-link" href="http://panfeng0119.github.io/post/">Archives</a>
          
        
      </li><li class="mobile-menu-item">
        
          
          
            <a class="menu-item-link" href="http://panfeng0119.github.io/tags/">Tags</a>
          
        
      </li><li class="mobile-menu-item">
        
          
          
            <a class="menu-item-link" href="http://panfeng0119.github.io/categories/">Categories</a>
          
        
      </li>
    
  </ul>
</nav>


  
    






  <link rel="stylesheet" href="/lib/photoswipe/photoswipe.min.css" />
  <link rel="stylesheet" href="/lib/photoswipe/default-skin/default-skin.min.css" />




<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

<div class="pswp__bg"></div>

<div class="pswp__scroll-wrap">
    
    <div class="pswp__container">
      <div class="pswp__item"></div>
      <div class="pswp__item"></div>
      <div class="pswp__item"></div>
    </div>
    
    <div class="pswp__ui pswp__ui--hidden">
    <div class="pswp__top-bar">
      
      <div class="pswp__counter"></div>
      <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
      <button class="pswp__button pswp__button--share" title="Share"></button>
      <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
      <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>
      
      
      <div class="pswp__preloader">
        <div class="pswp__preloader__icn">
          <div class="pswp__preloader__cut">
            <div class="pswp__preloader__donut"></div>
          </div>
        </div>
      </div>
    </div>
    <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
      <div class="pswp__share-tooltip"></div>
    </div>
    <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
    </button>
    <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
    </button>
    <div class="pswp__caption">
      <div class="pswp__caption__center"></div>
    </div>
    </div>
    </div>
</div>

  

  

  <header id="header" class="header container">
    <div class="logo-wrapper">
  <a href="/" class="logo">
    
      Pan Feng
    
  </a>
</div>

<nav class="site-navbar">
  <ul id="menu" class="menu">
    
    
        <li class="menu-item">
        
          
          
            <a class="menu-item-link" href="http://panfeng0119.github.io/">Home</a>
          

        

      </li>
    
        <li class="menu-item">
        
          
          
            <a class="menu-item-link" href="http://panfeng0119.github.io/post/">Archives</a>
          

        

      </li>
    
        <li class="menu-item">
        
          
          
            <a class="menu-item-link" href="http://panfeng0119.github.io/tags/">Tags</a>
          

        

      </li>
    
        <li class="menu-item">
        
          
          
            <a class="menu-item-link" href="http://panfeng0119.github.io/categories/">Categories</a>
          

        

      </li>
    
    
  </ul>
</nav>

  </header>

  <div id="mobile-panel">
    <main id="main" class="main bg-llight">
      <div class="content-wrapper">
        <div id="content" class="content container">
          <article class="post bg-white">
    
    <header class="post-header">
      <h1 class="post-title">[深度强化学习概述]A Brief Survey of Deep Reinforcement Learning</h1>
      
      <div class="post-meta">
        <time datetime="2018-12-26" class="post-time">
          2018-12-26
        </time>
        <div class="post-category">
            <a href="http://panfeng0119.github.io/categories/reinforcement-learning/"> reinforcement learning </a>
            
          </div>
        <span class="more-meta"> 2023 words </span>
          <span class="more-meta"> 5 min read </span>

        
        

        
        
      </div>
    </header>

    
    
<div class="post-toc" id="post-toc">
  <h2 class="post-toc-title">Table of Contents</h2>
  <div class="post-toc-content">
    <nav id="TableOfContents">
<ul>
<li><a href="#深度强化学习概述-a-brief-survey-of-deep-reinforcement-learning">[深度强化学习概述] A Brief Survey of Deep Reinforcement Learning</a>
<ul>
<li><a href="#摘要">摘要</a></li>
<li><a href="#1-introduction">1.INTRODUCTION</a>
<ul>
<li><a href="#1-1-什么是经验驱动的自主学习">1.1.什么是经验驱动的自主学习？</a></li>
<li><a href="#1-2-强化学习优缺点">1.2.强化学习优缺点？</a></li>
<li><a href="#1-3-为什么用深度学习">1.3.为什么用深度学习？</a></li>
<li><a href="#1-4-本文的目标是什么">1.4.本文的目标是什么？</a></li>
<li><a href="#1-5-drl-最近研究介绍">1.5.DRL 最近研究介绍</a>
<ul>
<li><a href="#1-开创先河的">（1）开创先河的</a></li>
<li><a href="#2-混合drl的系统-alphago">（2）混合DRL的系统 AlphaGo</a></li>
</ul></li>
</ul></li>
<li><a href="#ii-reward-driven-behaviour">II. REWARD-DRIVEN BEHAVIOUR</a>
<ul>
<li><a href="#2-1-rl的一般介绍">2.1.RL的一般介绍</a></li>
<li><a href="#2-2-最佳行动顺序由环境提供的奖励决定">2.2.最佳行动顺序由环境提供的奖励决定</a>
<ul>
<li><a href="#a-markov-decision-processes"><em>A. Markov Decision Processes</em></a></li>
</ul></li>
</ul></li>
</ul></li>
</ul>
</nav>
  </div>
</div>

    
    <div class="post-content">
      

<h1 id="深度强化学习概述-a-brief-survey-of-deep-reinforcement-learning">[深度强化学习概述] A Brief Survey of Deep Reinforcement Learning</h1>

<p>原文参考: arXiv:1708.05866v2 [cs.LG] 28 Sep 2017</p>

<h2 id="摘要">摘要</h2>

<p>本文主要内容有：</p>

<ol>
<li><p>研究深度强化学习的核心算法</p>

<ul>
<li>deep Q-network (深度Q网络)</li>
<li>trust region policy optimisation (置信区间策略优化)</li>
<li>asynchronous advantage actor-critic (异步优势 actor-critic算法)</li>
</ul></li>

<li><p>介绍深度神经网络的优势，主要关注通过强化学习来进行视觉理解</p></li>

<li><p>介绍一些当前几个领域内的研究</p></li>
</ol>

<h2 id="1-introduction">1.INTRODUCTION</h2>

<h3 id="1-1-什么是经验驱动的自主学习">1.1.什么是经验驱动的自主学习？</h3>

<p>人工智能领域主要目标之一就是生产出完全自主的agents，这种自主包括</p>

<ul>
<li>与环境互动，从中学习最佳行为</li>
<li>随时间的推移，通过<code>尝试</code>和<code>错误</code>来自我升级</li>
</ul>

<p>也就是说，这是一种以经验驱动的自主学习</p>

<h3 id="1-2-强化学习优缺点">1.2.强化学习优缺点？</h3>

<p><strong>强化学习（reinforcement learning, RL）</strong>就是这种基于原则的数学框架</p>

<p>但是它存在以下问题：</p>

<ul>
<li>缺乏可扩展性</li>
<li>受限于低维度的问题</li>
</ul>

<p>原因是 RL 与其他算法共享复杂度：存储器复杂性，计算复杂性，以及在这种情况下的机器学习算法，样本复杂性（sample complexity）</p>

<h3 id="1-3-为什么用深度学习">1.3.为什么用深度学习？</h3>

<p>近些年，深度学习提高了对象检测、语音识别和语言翻译等任务的最新技术水平</p>

<p>DL 最重要的特性是能够自动地找到紧凑的低维表示特征来表示高维的数据（e.g., images, text and audio）</p>

<p>深度学习具有强大的函数逼近（powerful function approximation ）和表示学习属性（representation learning properties ），是克服上面这些问题的实用的方法</p>

<p>深度学习可以加速RL的进步，在RL中使用深度学习算法定义了“深度强化学习”（DRL）领域</p>

<h3 id="1-4-本文的目标是什么">1.4.本文的目标是什么？</h3>

<p>本文目标是整理开创性和最新的 DRL, 整理出哪种神经网络更能用于自主agents（autonomous ）</p>

<p>有关近期更全面的 DRL 调查，包括DRL在自然语言处理等领域的应用[106,5]，我们向读者推荐Li [78]</p>

<h3 id="1-5-drl-最近研究介绍">1.5.DRL 最近研究介绍</h3>

<p>在最近DRL领域的工作中，有两个杰出的成功案例。</p>

<h4 id="1-开创先河的">（1）开创先河的</h4>

<p>[84]开发一种算法能够玩一系列的 Atari 2600 视频游戏，并且能在<strong>超人（superhuman）</strong>难度下进行。它为 RL 在函数逼近技术上不稳定问题提供了一种解决方案。首次证明了 RL agents 能仅仅通过奖励信号就可以在原始高维的观测数据上学习</p>

<h4 id="2-混合drl的系统-alphago">（2）混合DRL的系统 AlphaGo</h4>

<p>AlphaGo[128]击败人类世界冠军，与二十年前国际象棋中IBM的Deep Blue[19]以及IBM的Watson DeepQA系统（击败了最牛逼的Jeopardy！玩家）[31]的历史性成就相媲美</p>

<p>AlphaGo是由经过监督和强化学习训练的神经网络组成，并结合传统的启发式搜索算法</p>

<p>现在的机器人控制策略可以直接从现实世界的摄像机输入学习[74,75]，后续的调节器（controllers）用手工设计或从状态的低维特征中学习。</p>

<p>进一步的，DRL已被用于元学习（“学习如何学习”）[29,156]，使之通用到以前从未见过的复杂视觉环境[29]。在图1中，我们展示了DRL应用的一些领域，从玩视频游戏[84]到室内导航[167]。</p>

<p>DRL背后的驱动力之一是创建能够学习如何适应现实世界的系统。 从管理功耗[142]到挑选并放入物体[75]，DRL可以增加可以通过学习自动化的物理任务量。然而，DRL并不止于此，因为RL是通过反复试验来解决优化问题的一般方法。 从设计最先进的机器翻译模型[168]到构建新的优化功能[76]，DRL已经被用于处理各种机器学习任务。</p>

<h2 id="ii-reward-driven-behaviour">II. REWARD-DRIVEN BEHAVIOUR</h2>

<h3 id="2-1-rl的一般介绍">2.1.RL的一般介绍</h3>

<p>RL 的本质是通过互动进行学习，一个 RL agent 与它所在的环境进行互动。并在观察其行为序列的基础上，可以学习改变其自身的行为来所获得的奖励。这种学习 trial 和 error 的范式（paradigm）源自于行为主义心理学（behaviourist psychology），这也是 RL 的主要基础之一[135]</p>

<p>RL 另一个关键影响是优化控制，它提供了支撑该领域的数学形式（最值得注意的是动态编程[13]）</p>

<p>在RL的set-up过程中，由机器学习算法控制的自主 <em>agent</em> 从环境中观察它所处的时间 timestep <em>t</em> 下的状态 <em>state</em> <code>$s_{t}$</code></p>

<p><em>agent</em> 与环境交互方式为在 <code>$s_{t}$</code> 下执行一个 <em>action</em> <code>$a_{t}$</code></p>

<p>当 <em>agent</em> 执行 <code>$a_{t}$</code> 后，环境和agent会转换到 <code>$s_{t+1}$</code> 的新状态</p>

<p>状态是对环境的一个充分的统计，因此它包括aget执行最佳action的所有必要信息，它可以包含部分的agent，如执行器和传感器的位置信息</p>

<p>在优化控制方面的文献中，状态和活动经常被表示为 $x_t$ 和 $u_t$</p>

<h3 id="2-2-最佳行动顺序由环境提供的奖励决定">2.2.最佳行动顺序由环境提供的奖励决定</h3>

<p>每次环境转换到新状态时，会向agent提供相应的奖励 $r_{t + 1}$ 作为反馈。而agent的目标是学习一种政策 $π$ ，使这种预期收益（累积，折现奖励）最大化。</p>

<p>在这方面，RL旨在解决与最优控制相同的问题。</p>

<p>然而，RL中的挑战是agent需要通过反复试验来了解环境中行为的后果，而优化控制的动态转换状态模型对agent不可用。 与环境的每次交互都会产生信息，代理会使用这些信息来更新其知识。 这种感知 - 动作 - 学习循环。</p>

<h4 id="a-markov-decision-processes"><em>A. Markov Decision Processes</em></h4>

<p>Formally, RL can be described as a Markov decision process (MDP), which consists of:</p>

<ul>
<li>A set of states S, plus a distribution of starting states p(s 0 ).</li>
<li>A set of actions A.</li>
<li>Transition dynamics T (s t+1 |s t , a t ) that map a state- action pair at time t onto a distribution of states at time t + 1.</li>
<li>An immediate/instantaneous reward function R(s t , a t , s t+1 ).</li>
<li>A discount factor γ ∈ [0, 1], where lower values place more emphasis on immediate rewards.</li>
</ul>

    </div>

    
    
<div class="post-copyright">
  <p class="copyright-item">
    <span class="item-title">Author</span>
    <span class="item-content">Pan Feng</span>
  </p>
  <p class="copyright-item">
    <span class="item-title">LastMod</span>
    <span class="item-content">2018-12-26</span>
  </p>
  
  <p class="copyright-item">
    <span class="item-title">License</span>
    <span class="item-content">None</span>
  </p>
</div>


    
    

    <footer class="post-footer">
      <div class="post-tags">
          <a href="http://panfeng0119.github.io/tags/reinforcement-learning/">reinforcement learning</a>
          
        </div>

      
      <nav class="post-nav">
        
          <a class="prev" href="/post/reinforcement-learning-2018-12-26-model-based-deep-reinforcement-learning/">
            
            <i class="iconfont">
              <svg  class="icon" viewBox="0 0 1024 1024" version="1.1"
  xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
  width="18" height="18">
  <path d="M691.908486 949.511495l75.369571-89.491197c10.963703-12.998035 10.285251-32.864502-1.499144-44.378743L479.499795 515.267417 757.434875 204.940602c11.338233-12.190647 11.035334-32.285311-0.638543-44.850487l-80.46666-86.564541c-11.680017-12.583596-30.356378-12.893658-41.662889-0.716314L257.233596 494.235404c-11.332093 12.183484-11.041474 32.266891 0.657986 44.844348l80.46666 86.564541c1.772366 1.910513 3.706415 3.533476 5.750981 4.877077l306.620399 321.703933C662.505829 963.726242 680.945807 962.528973 691.908486 949.511495z"></path>
</svg>

            </i>
            <span class="prev-text nav-default">[模型强化学习PPT]Model-Based Deep Reinforcement Learning</span>
            <span class="prev-text nav-mobile">Prev</span>
          </a>
        
          <a class="next" href="/post/leetcode/leetcode-006/">
            <span class="next-text nav-default">LeetCode 刷题笔记06</span>
            <span class="prev-text nav-mobile">Next</span>
            
            <i class="iconfont">
              <svg class="icon" viewBox="0 0 1024 1024" version="1.1"
  xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
  width="18" height="18">
  <path d="M332.091514 74.487481l-75.369571 89.491197c-10.963703 12.998035-10.285251 32.864502 1.499144 44.378743l286.278095 300.375162L266.565125 819.058374c-11.338233 12.190647-11.035334 32.285311 0.638543 44.850487l80.46666 86.564541c11.680017 12.583596 30.356378 12.893658 41.662889 0.716314l377.434212-421.426145c11.332093-12.183484 11.041474-32.266891-0.657986-44.844348l-80.46666-86.564541c-1.772366-1.910513-3.706415-3.533476-5.750981-4.877077L373.270379 71.774697C361.493148 60.273758 343.054193 61.470003 332.091514 74.487481z"></path>
</svg>

            </i>
          </a>
      </nav>
    </footer>
  </article>

  
  
  
  

  
  

  
  

  

  

  
  
    



        </div>
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="social-links">
  






</div>

<div class="copyright">
  <span class="power-by">
    Powered by <a class="hexo-link" href="https://gohugo.io">Hugo</a>
  </span>
  <span class="division">|</span>
  <span class="theme-info">
    Theme - <a class="theme-link" href="https://github.com/xianmin/hugo-theme-jane">Jane</a>
  </span>

  <span class="copyright-year">
    &copy;
    
      2018 -
    2019
    <span class="heart">
      
      <i class="iconfont">
        <svg class="icon" viewBox="0 0 1025 1024" version="1.1"
  xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
  width="14" height="14">
  <path d="M1000.1 247.9c-15.5-37.3-37.6-70.6-65.7-98.9-54.4-54.8-125.8-85-201-85-85.7 0-166 39-221.4 107.4C456.6 103 376.3 64 290.6 64c-75.1 0-146.5 30.4-201.1 85.6-28.2 28.5-50.4 61.9-65.8 99.3-16 38.8-24 79.9-23.6 122.2 0.7 91.7 40.1 177.2 108.1 234.8 3.1 2.6 6 5.1 8.9 7.8 14.9 13.4 58 52.8 112.6 102.7 93.5 85.5 209.9 191.9 257.5 234.2 7 6.1 15.8 9.5 24.9 9.5 9.2 0 18.1-3.4 24.9-9.5 34.5-30.7 105.8-95.9 181.4-165 74.2-67.8 150.9-138 195.8-178.2 69.5-57.9 109.6-144.4 109.9-237.3 0.1-42.5-8-83.6-24-122.2z"
   fill="#8a8a8a"></path>
</svg>

      </i>
    </span><span class="author">
        Pan Feng
        
      </span></span>

  
  
</div>

    </footer>

    <div class="back-to-top" id="back-to-top">
      <i class="iconfont">
        
        <svg class="icon" viewBox="0 0 1024 1024" version="1.1"
  xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
  width="35" height="35">
  <path d="M510.866688 227.694839 95.449397 629.218702l235.761562 0-2.057869 328.796468 362.40389 0L691.55698 628.188232l241.942331-3.089361L510.866688 227.694839zM63.840492 63.962777l894.052392 0 0 131.813095L63.840492 195.775872 63.840492 63.962777 63.840492 63.962777zM63.840492 63.962777"></path>
</svg>

      </i>
    </div>
  </div>
  
<script type="text/javascript" src="/lib/jquery/jquery-3.2.1.min.js"></script>
  <script type="text/javascript" src="/lib/slideout/slideout-1.0.1.min.js"></script>




<script type="text/javascript" src="/js/main.638251f4230630f0335d8c6748e53a96f94b72670920b60c09a56fdc8bece214.js" integrity="sha256-Y4JR9CMGMPAzXYxnSOU6lvlLcmcJILYMCaVv3Ivs4hQ=" crossorigin="anonymous"></script>

  <script type="text/javascript">
    window.MathJax = {
      showProcessingMessages: false,
      messageStyle: 'none'
    };
  </script>
  
  

  

  
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [['$','$'], ['\\(','\\)']],
        displayMath: [['$$','$$'], ['\[[','\]]']],
        processEscapes: true,
        processEnvironments: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
        TeX: { equationNumbers: { autoNumber: "AMS" },
                extensions: ["AMSmath.js", "AMSsymbols.js"] }
    }
});
</script>
  
<script type="text/x-mathjax-config">
    // Fix <code> tags after MathJax finishes running. This is a
    // hack to overcome a shortcoming of Markdown. Discussion at
    // https://github.com/mojombo/jekyll/issues/199
    MathJax.Hub.Queue(() => {
    MathJax.Hub.getAllJax().map(v => v.SourceElement().parentNode.className += ' has-jax');
    });
</script> 

<style>
    code.has-jax {
        font: inherit;
        font-size: 100%;
        background: inherit;
        border: inherit;
        color: #515151;
    }
</style>





  
    <script type="text/javascript" src="/js/load-photoswipe.js"></script>
    <script type="text/javascript" src="/lib/photoswipe/photoswipe.min.js"></script>
    <script type="text/javascript" src="/lib/photoswipe/photoswipe-ui-default.min.js"></script>
  













</body>
</html>
